{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Seminararbeit**\n",
    "von Leon Lantz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ðŸš€ Bibliotheken-Import und CUDA-VerfÃ¼gbarkeit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonl\\anaconda3\\envs\\stable_diffusion\\lib\\site-packages\\diffusers\\models\\transformers\\transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda verfÃ¼gbar? --> True\n",
      "NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "import shutil\n",
    "import json\n",
    "import mediapipe as mp\n",
    "from torch import autocast\n",
    "import random\n",
    "import cv2\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "print(\"Cuda verfÃ¼gbar? -->\", torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ðŸ–¼ï¸ COCO Dataset** \n",
    "https://cocodataset.org/\n",
    "\n",
    "- mehr als 200.000 reale Bilder aus unterschiedlichsten Szenarien\n",
    "- unterteilt in 80 Kategorien (Personen, Fahrzeuge, Tiere, ...)\n",
    "- jedes Bild detailliert annotiert, unter anderem mit Bildunterschriften (Captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordner-Pfade\n",
    "results_path  = 'results/'\n",
    "images_path = 'coco_dataset/train2017'\n",
    "annotations_path = 'coco_dataset/annotations/instances_train2017.json'\n",
    "captions_path = 'coco_dataset/annotations/captions_train2017.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lade die COCO-Annotationen\n",
    "with open(annotations_path, 'r') as f:\n",
    "    coco_annotations = json.load(f)\n",
    "\n",
    "# Lade die COCO-Annotationen\n",
    "with open(captions_path, 'r') as f:\n",
    "    coco_captions = json.load(f)\n",
    "\n",
    "\n",
    "# Erstelle ein Mapping fÃ¼r ID und Bild-Pfad\n",
    "mapping_filename = {}\n",
    "for image in coco_annotations['images']:\n",
    "    mapping_filename[image['id']] = image['file_name']\n",
    "\n",
    "def get_filename_from_image_id(image_id):\n",
    "    return mapping_filename.get(image_id, \"Bild-ID nicht gefunden\")\n",
    "\n",
    "\n",
    "# Erstelle ein leeres Dictionary fÃ¼r das Mapping\n",
    "image_id_to_caption = {}\n",
    "\n",
    "# Durchlaufe die Liste der Captions und fÃ¼lle das Mapping\n",
    "for annotation in coco_captions['annotations']:\n",
    "    image_id = annotation['image_id']\n",
    "    caption = annotation['caption']\n",
    "    if image_id not in image_id_to_caption:\n",
    "        image_id_to_caption[image_id] = [caption]\n",
    "    else:\n",
    "        image_id_to_caption[image_id].append(caption)\n",
    "\n",
    "def get_caption_from_image_id(image_id):\n",
    "    captions_list = image_id_to_caption.get(image_id, \"Bild-ID nicht gefunden\")\n",
    "    \n",
    "    # Sortiere die Bildunterschriften nach der LÃ¤nge der WÃ¶rter\n",
    "    sorted_captions = sorted(captions_list, key=lambda x: len(x.split()), reverse=True)\n",
    "    \n",
    "    # Gib die erste (detaillierteste) Bildunterschrift zurÃ¼ck\n",
    "    return sorted_captions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verwendung des COCO-Helpers um nur Bilder der Kategorie \"Person\" herauszufiltern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=9.65s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Initialisiere COCO-Helper\n",
    "coco = COCO(annotations_path)\n",
    "\n",
    "# Hole alle Kategorien\n",
    "categories = coco.loadCats(coco.getCatIds())\n",
    "category_names = [cat['name'] for cat in categories]\n",
    "\n",
    "# Finde die ID der Kategorie \"person\" --> Nur Bilder die im Dataset bereits als \"Person\" markiert\n",
    "category_id = coco.getCatIds(catNms=['person'])\n",
    "\n",
    "# Hole alle Annotationen der Kategorie \"person\"\n",
    "ann_ids = coco.getAnnIds(catIds=category_id)\n",
    "anns = coco.loadAnns(ann_ids)\n",
    "\n",
    "# Hole die Bild-IDs der Annotationen\n",
    "image_ids = list(set([ann['image_id'] for ann in anns]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ðŸ™‚ Gesichtserkennung**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectFace(img_name):\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "    def get_rectangle(objDictionary, ss):\n",
    "        left = int(objDictionary.xmin * ss[1])\n",
    "        top = int(objDictionary.ymin * ss[0])\n",
    "        right = int(left + objDictionary.width * ss[1])\n",
    "        bottom = int(top + objDictionary.height * ss[0])\n",
    "        return ((left, top), (right, bottom))\n",
    "\n",
    "    with mp_face_detection.FaceDetection(model_selection=3, min_detection_confidence=0.8) as face_detection:\n",
    "        image = cv2.imread(img_name)\n",
    "        results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    if not results.detections:\n",
    "        return None\n",
    "\n",
    "    faces = []\n",
    "\n",
    "    for obj in results.detections:\n",
    "        ff = get_rectangle(obj.location_data.relative_bounding_box, image.shape)\n",
    "        (left, top), (right, bottom) = ff\n",
    "        faces.append(image[top:bottom, left:right])\n",
    "\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_detected_faces(folder_name):\n",
    "  files = os.listdir(folder_name)\n",
    "\n",
    "  r_count = 0\n",
    "  for f_name in files:\n",
    "    faces = detectFace(os.path.join(folder_name,f_name))\n",
    "    if not faces: \n",
    "      os.remove(os.path.join(folder_name,f_name))\n",
    "      r_count += 1\n",
    "  \n",
    "  return r_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **âœ‚ï¸ Extrahieren von realen Gesichtern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_faces(img_path, save_path, num_faces_wanted, face_res = 100):\n",
    "\n",
    "  if os.path.exists(save_path):\n",
    "    shutil.rmtree(save_path)\n",
    "  os.mkdir(save_path)\n",
    "\n",
    "  count = 0\n",
    "\n",
    "  for img_id in random.sample(image_ids, num_faces_wanted*10):\n",
    "\n",
    "    if count >= num_faces_wanted: break\n",
    "\n",
    "    f_name = get_filename_from_image_id(img_id)\n",
    "\n",
    "    faces = detectFace(os.path.join(img_path,f_name))\n",
    "    if not faces: continue\n",
    "\n",
    "    for face in faces:\n",
    "      if not face.size: continue\n",
    "      face = cv2.resize(face,(face_res,face_res))\n",
    "      cv2.imwrite(os.path.join((f'{save_path}'), str(count)+'.jpeg'), face)\n",
    "      count += 1      \n",
    "\n",
    "  return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonl\\anaconda3\\envs\\stable_diffusion\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Gesichter wurden im Ordner realFaces0 generiert.\n",
      "193 Gesichter wurden entfernt.\n"
     ]
    }
   ],
   "source": [
    "num_runs = 2\n",
    "num_faces_wanted = 1000\n",
    "face_res = 100\n",
    "\n",
    "for n in range(num_runs):\n",
    "    faces_generated = find_faces(images_path, f'{results_path}/realFaces{n}', num_faces_wanted, face_res) # type: ignore\n",
    "    print(f'{faces_generated} Gesichter wurden im Ordner realFaces{n} generiert.')\n",
    "    x = prune_detected_faces((f'{results_path}/realFaces{n}')) # type: ignore\n",
    "    print(f'{x} Gesichter wurden entfernt.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ðŸ”„ Generierung mit Stable Diffusion v1-4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonl\\anaconda3\\envs\\stable_diffusion\\lib\\site-packages\\diffusers\\pipelines\\pipeline_loading_utils.py:219: FutureWarning: You are loading the variant fp16 from CompVis/stable-diffusion-v1-4 via `revision='fp16'` even though you can load it via `variant=`fp16`. Loading model variants via `revision='fp16'` is deprecated and will be removed in diffusers v1. Please use `variant='fp16'` instead.\n",
      "  warnings.warn(\n",
      "unet\\diffusion_pytorch_model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec076e5c7f94b5e842e530d83f9388a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonl\\anaconda3\\envs\\stable_diffusion\\lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def select_model_pipeline(name):\n",
    "    if name == \"stable-diffusion-v1-4\":\n",
    "        # Instanziiere eine Stable Diffusion Pipeline aus dem Modell \"CompVis/stable-diffusion-v1-4\"\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16)  \n",
    "        pipe.to(\"cuda\")\n",
    "    elif name == \"stable-diffusion-xl-base-1.0\":\n",
    "        # Instanziiere eine Stable Diffusion Pipeline aus dem Modell \"CompVis/stable-diffusion-xl-base-1.0\"\n",
    "        pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
    "        pipe.to(\"cuda\")\n",
    "    \n",
    "    return pipe  # RÃ¼ckgabe der Pipeline am Ende der Funktion\n",
    "\n",
    "pipe = select_model_pipeline(\"stable-diffusion-v1-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a202e9ff8c94374a775a22c900ac2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionXLPipeline {\n",
       "  \"_class_name\": \"StableDiffusionXLPipeline\",\n",
       "  \"_diffusers_version\": \"0.28.2\",\n",
       "  \"_name_or_path\": \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
       "  \"feature_extractor\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"force_zeros_for_empty_prompt\": true,\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"EulerDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"text_encoder_2\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModelWithProjection\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"tokenizer_2\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16)  \n",
    "pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_faces_generated(save_path_images, save_path_faces, num_faces_wanted, face_res=100):\n",
    "\n",
    "  if os.path.exists(save_path_images):\n",
    "    shutil.rmtree(save_path_images)    \n",
    "  os.mkdir(save_path_images)\n",
    "\n",
    "  if os.path.exists(save_path_faces):\n",
    "    shutil.rmtree(save_path_faces)    \n",
    "  os.mkdir(save_path_faces)\n",
    "\n",
    "  count_f = 0\n",
    "  count_i = 0\n",
    "\n",
    "  for img_id in random.sample(image_ids, num_faces_wanted*10):\n",
    "\n",
    "    if count_f >= num_faces_wanted: break\n",
    "\n",
    "    caption = get_caption_from_image_id(img_id)\n",
    "    caption += \". Enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. Ensure clear and detailed facial expressions for a captivating result.\"\n",
    "    prompt = caption.lower()\n",
    "    print(prompt)\n",
    "\n",
    "    # generate image\n",
    "    with autocast(\"cuda\"):\n",
    "      image = pipe(prompt).images[0] \n",
    "      image.save(f'{save_path_images}/{count_i}.jpeg')\n",
    "      count_i += 1\n",
    "\n",
    "    faces = detectFace(os.path.join(f'{save_path_images}/', str(count_i-1)+'.jpeg'))\n",
    "    if not faces: continue\n",
    "\n",
    "    for face in faces:\n",
    "      if not face.size: continue\n",
    "      face = cv2.resize(face,(face_res,face_res))\n",
    "      cv2.imwrite(os.path.join(f'{save_path_faces}/', str(count_f)+'.jpeg'), face)\n",
    "      count_f += 1     \n",
    "\n",
    "  return count_i, count_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LÃ¶sche den GPU-Cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a picture of two giraffes, fairly close to a road, with a bus traveling up it.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7625e1d83aa2439288f413c7c3182ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a woman in a blue shirt sitting on a park bench next to a large backpack.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e171c50c4fa4e5f8868895b97bba5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a parking lot with several buses and a few cars parked there.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949a66e2a68f4930abec085607a54390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two cross country skiers make their way along a snow covered trail between the evergreen trees.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b87deb3d6744fdf9ca3fd8f4333bbbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a woman reaching for a piece of bread in a oven.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010bf342a30547d0b97e2745468b20b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up close picture of baseball batter wearing gloves and helmet.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2620db913b3c4381983eff539cbc25ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two men in red coats on horse back with a third man waking behind in the same type of coat.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2a06ab5add4d2ebfcd3f37d9f3b158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two glasses or wine, one red and one white, sit with two bottles of wine while people mingle in the background.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c4236ba7bb4f13b6f2bd12d3d34ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a couple of kids are dancing together in a room. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767f116e7ab74698965cbd38b141402a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a black and white photo of a skateboarder skating with people watching.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e63e33c5b84c3c8d71c6b35f8e8333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a family posing in front of a old bus. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed19096b0a844aed99f2bfc16b1403e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a black cow and calf with a little boy standing by a ugly building.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65aa2cc62d546d89b5ac6eeefffdac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man on a snowboard with goggles on, and other people on skis behind him on a snow covered ground area.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333bd4efa4234707b8264490a68318b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man taking picture of another man standing next to a baseball player.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d199621dcb9e47988c345a33d3d9bb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a skate boarder is doing a trick on the side of an empty pool.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbd40a8fc6645528a6d9171034cdfc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man has hit the baseball and is preparing to run the bases.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1628d1aa424d6cbe7de38fdae2c109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a young boy holds a tennis racket on a tennis court.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be18054e96b1446e8fd732f8a8abf8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the street sign has on outline of a man with a question mark on it. . enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6396d7ef247541c0bda73fcfac4d6c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man in a bicycle helmet holding a cup of ice cream.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bc3e7480684a0d855ea633e7c122d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a girl sitting and smiling at a desk with a computer.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6522b2244e9b4e44a8f9f6bd59554c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man sitting at a table about to enjoy a bowl of steamed broccoli.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798e16fe83054ff391f15e588231c775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tows tied up to a fence and a man walking on a sidewalk with a sack on his head. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c04e22360e242339ca234b47c4a12a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man wearing an orange vest. taking items from the back of a white truck. . enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8ce32741c74fb8800ae6bab0df355f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man stands in front of a plane in black and white. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c118691aa85a414fbb8f361ec1856401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little children running and playing in a soccer match as a parent watches. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca992bb00194d1bb2409b498ea339e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a sub sandwich made with chicken and tomato.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83fb867a7e5b41b29eaeccea54d80cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "young man holding a tennis racket while standing in front of a fence.. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31ba01c8bd14ff4ab2758be882e3ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two young people in their ski gear are posing for the camera. . enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64900f4ffbf94adcbeffc1fc1cf9c3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a couple of people are riding on the back of horses. enhance the image to emphasize facial features, such as expressive eyes and radiant smiles. ensure clear and detailed facial expressions for a captivating result.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19e70d020b94cdfa1c00bbb6d14392c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m face_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_runs):\n\u001b[1;32m----> 7\u001b[0m   images_generated, faces_generated \u001b[38;5;241m=\u001b[39m \u001b[43mfind_faces_generated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mresults_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/imagesGenerated\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mn\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mresults_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/facesGenerated\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mn\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_faces_wanted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfaces_generated\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m faces were generated in folder \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/facesGenerated\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages_generated\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m faces were generated in folder \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/imagesGenerated\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 25\u001b[0m, in \u001b[0;36mfind_faces_generated\u001b[1;34m(save_path_images, save_path_faces, num_faces_wanted, face_res)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# generate image\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 25\u001b[0m   image \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m] \n\u001b[0;32m     26\u001b[0m   image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path_images\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m   count_i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\leonl\\anaconda3\\envs\\stable_diffusion\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leonl\\anaconda3\\envs\\stable_diffusion\\lib\\site-packages\\diffusers\\pipelines\\stable_diffusion\\pipeline_stable_diffusion.py:1025\u001b[0m, in \u001b[0;36mStableDiffusionPipeline.__call__\u001b[1;34m(self, prompt, height, width, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguidance_rescale)\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# compute the previous noisy sample x_t -> x_t-1\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep(noise_pred, t, latents, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_step_kwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callback_on_step_end \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1028\u001b[0m     callback_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\leonl\\anaconda3\\envs\\stable_diffusion\\lib\\site-packages\\diffusers\\schedulers\\scheduling_pndm.py:257\u001b[0m, in \u001b[0;36mPNDMScheduler.step\u001b[1;34m(self, model_output, timestep, sample, return_dict)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_prk(model_output\u001b[38;5;241m=\u001b[39mmodel_output, timestep\u001b[38;5;241m=\u001b[39mtimestep, sample\u001b[38;5;241m=\u001b[39msample, return_dict\u001b[38;5;241m=\u001b[39mreturn_dict)\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_plms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leonl\\anaconda3\\envs\\stable_diffusion\\lib\\site-packages\\diffusers\\schedulers\\scheduling_pndm.py:382\u001b[0m, in \u001b[0;36mPNDMScheduler.step_plms\u001b[1;34m(self, model_output, timestep, sample, return_dict)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    380\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m24\u001b[39m) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m55\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mets[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m59\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mets[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m37\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mets[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m9\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mets[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m])\n\u001b[1;32m--> 382\u001b[0m prev_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_prev_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_timestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "File \u001b[1;32mc:\\Users\\leonl\\anaconda3\\envs\\stable_diffusion\\lib\\site-packages\\diffusers\\schedulers\\scheduling_pndm.py:419\u001b[0m, in \u001b[0;36mPNDMScheduler._get_prev_sample\u001b[1;34m(self, sample, timestep, prev_timestep, model_output)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_prev_sample\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample, timestep, prev_timestep, model_output):\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# See formula (9) of PNDM paper https://arxiv.org/pdf/2202.09778.pdf\u001b[39;00m\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;66;03m# this function computes x_(tâˆ’Î´) using the formula of (9)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# model_output -> e_Î¸(x_t, t)\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;66;03m# prev_sample -> x_(tâˆ’Î´)\u001b[39;00m\n\u001b[0;32m    418\u001b[0m     alpha_prod_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas_cumprod[timestep]\n\u001b[1;32m--> 419\u001b[0m     alpha_prod_t_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas_cumprod[prev_timestep] \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mprev_timestep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_alpha_cumprod\n\u001b[0;32m    420\u001b[0m     beta_prod_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha_prod_t\n\u001b[0;32m    421\u001b[0m     beta_prod_t_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha_prod_t_prev\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_runs = 1\n",
    "processed_images = set()\n",
    "num_faces_wanted = 250\n",
    "face_res = 100\n",
    "\n",
    "for n in range(num_runs):\n",
    "  images_generated, faces_generated = find_faces_generated(f'{results_path}/imagesGenerated{n}', f'{results_path}/facesGenerated{n}', num_faces_wanted, face_res)\n",
    "  print(f'{faces_generated} faces were generated in folder {results_path}/facesGenerated{n}')\n",
    "  print(f'{images_generated} faces were generated in folder {results_path}/imagesGenerated{n}')\n",
    "  x = prune_detected_faces((f'{results_path}/facesGenerated{n}'))\n",
    "  print(f'{x} faces were removed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable_diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
